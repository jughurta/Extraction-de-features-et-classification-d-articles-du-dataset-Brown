{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news :\n",
      "   88 documents\n",
      "literature :\n",
      "   88 documents\n",
      "sciences :\n",
      "   80 documents\n",
      "NB instances : 256\n",
      "On prend 17 éléments sur 88 pour le test\n",
      "On prend 17 éléments sur 88 pour le test\n",
      "On prend 16 éléments sur 80 pour le test\n",
      "Dataset stocké dans train_test.json\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import my_toolsv2 as mt\n",
    "import json\n",
    "\n",
    "def ex_constitution_corpus():\n",
    "  themes = {\"news\":[\"news\", \"reviews\", \"editorial\"], \n",
    "            \"literature\":[\"science_fiction\", \"romance\", \"fiction\", \"mystery\"], \n",
    "            \"sciences\":[\"learned\"]}\n",
    "  nb_instances = 0\n",
    "  corpus = {}\n",
    "  for category in themes:\n",
    "    print(category, \":\")\n",
    "    nb_doc = len(brown.fileids(categories=themes[category]))\n",
    "    print(\"  \",nb_doc, \"documents\")\n",
    "    nb_instances += nb_doc\n",
    "    corpus[category] = brown.fileids(categories=themes[category])\n",
    "  print(\"NB instances :\", nb_instances)\n",
    "  return corpus\n",
    "\n",
    "def get_train_test_corpus(corpus):\n",
    "  import random\n",
    "  train = {}\n",
    "  test = {}\n",
    "  for category, fileids in corpus.items():\n",
    "    x = int(20*len(fileids)/100)#nb instances pour le train set\n",
    "    test[category] = []\n",
    "    print(\"On prend %s éléments sur %s pour le test\"%(str(x),str(len(fileids))))\n",
    "    for i in range(x):\n",
    "      id_doc = random.randint(0,len(fileids)-1)#prend un index au hasard\n",
    "      test[category].append(fileids[id_doc])#stocke le document dans test\n",
    "      fileids.remove(fileids[id_doc])\n",
    "    train[category] = fileids #le reste va dans le train set\n",
    "  dataset = {\"train\": train, \"test\":test}\n",
    "  return dataset\n",
    "\n",
    "###Corpus complet\n",
    "corpus = ex_constitution_corpus()\n",
    "\n",
    "###Séparation train/test\n",
    "test_train = get_train_test_corpus(corpus)\n",
    "\n",
    "###Stockage du résultat\n",
    "test_train_json = json.dumps(test_train, indent =2)\n",
    "chemin = \"train_test.json\"\n",
    "mt.ecrire(test_train_json, chemin)\n",
    "print(\"Dataset stocké dans %s\"%chemin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Récupération de la liste des fichiers\n",
      "-> 256 fichiers\n",
      "\n",
      "Extraction des features\n",
      "->Features extraites: ['L_moyenne_mots', 'L_min_mots', 'L_max_mots', 'nb_affirm', 'nb_interro', 'nb_excla', 'Moyenne_mot_phrase'] ...\n",
      "\n",
      "Ecriture de la sortie JSON\n",
      "-> features_by_file.json\n"
     ]
    }
   ],
   "source": [
    "def get_features_light(liste_fichier):\n",
    "  features_file = {}\n",
    "  for fileid in liste_fichier:\n",
    "    features_file[fileid] = {}\n",
    "    stats_mots = mt.get_stats_longueur(brown.words(fileid))\n",
    "    for feature, valeur in stats_mots.items():\n",
    "      features_file[fileid][feature] = valeur\n",
    "    stats_phrases = mt.get_types_phrases(brown.raw(fileid))\n",
    "    for feature, valeur in stats_phrases.items():\n",
    "      features_file[fileid][feature] = valeur\n",
    "    mots = brown.words(fileid)\n",
    "    phrases = brown.sents(fileid)\n",
    "    texte = brown.raw(fileid)\n",
    "    \n",
    "    stats_longueur_phrases = mt.get_stats_longueur_phrases(mots, phrases)\n",
    "    for feature, valeur in stats_longueur_phrases.items():\n",
    "      features_file[fileid][feature] = valeur\n",
    "  print(\"->Features extraites:\", list(features_file[fileid].keys())[:20],\"...\")\n",
    "  return features_file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###On récupère la liste des fichiers à traiter\n",
    "train_test = json.load(open(\"train_test.json\"))\n",
    "print(\"\\nRécupération de la liste des fichiers\")\n",
    "liste_all_files = mt.get_all_files(train_test)\n",
    "print(\"-> %s fichiers\"%str(len(liste_all_files))) \n",
    "\n",
    "###On extrait les features\n",
    "print(\"\\nExtraction des features\")\n",
    "features_by_file = get_features_light(liste_all_files)\n",
    "\n",
    "\n",
    "\n",
    "###On écrit la sortie\n",
    "print(\"\\nEcriture de la sortie JSON\")\n",
    "filename = \"features_by_file.json\"\n",
    "mt.sauvegarder(features_by_file, filename)\n",
    "print(\"-> %s\"%filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dossier 'arff_files' déjà créé\n",
      "\n",
      "Feature set : all_features\n",
      "  Processing train set\n",
      "    ->sortie = arff_files/all_features__train.arff\n",
      "  Processing test set\n",
      "    ->sortie = arff_files/all_features__test.arff\n",
      "\n",
      "Feature set : Mots_only\n",
      "  Processing train set\n",
      "    ->sortie = arff_files/Mots_only__train.arff\n",
      "  Processing test set\n",
      "    ->sortie = arff_files/Mots_only__test.arff\n"
     ]
    }
   ],
   "source": [
    "              #### Phase 3 #####\n",
    "\n",
    "###Création des ARFF\n",
    "import os\n",
    "\n",
    "def get_entete_arff(feature_names, classes):\n",
    "  \"\"\"On crée l'entête du arff\"\"\"\n",
    "  lignes_arff = [\"@relation TRAIN_DATABASE\\n\"]#nom de la relation\n",
    "  for name in feature_names:\n",
    "    lignes_arff.append(\"@attribute %s numeric\\n\"%name)#noms des features\n",
    "  lignes_arff.append(\"@attribute classes {%s}\\n\"%\",\".join(classes))#les classes\n",
    "  return lignes_arff\n",
    "\n",
    "def get_lignes_arff(feature_names, classes, features_by_file):\n",
    "  \"\"\"Renvoie la liste des lignes nécessaires pour le fichier ARFF\"\"\"\n",
    "  lignes_arff = get_entete_arff(feature_names, classes)\n",
    "  lignes_arff.append(\"@data\\n\\n\")#on passe à la partie DATA\n",
    "  for nom_classe, l_fichier in classes.items():\n",
    "    for fileid in l_fichier:#Pour chaque fichier\n",
    "      l_values = [features_by_file[fileid][name] for name in feature_names]\n",
    "      l_values.append(nom_classe)#pour l'évaluation\n",
    "      # La liste des features séparées par des virgules :\n",
    "      ligne_values = \",\".join([str(x) for x in l_values])\n",
    "      lignes_arff.append(ligne_values)\n",
    "\n",
    "  return lignes_arff\n",
    "\n",
    "##pour avoir la répartition train_test\n",
    "train_test =mt.lire_json(\"train_test.json\")\n",
    "\n",
    "##la liste des fichiers à traiter\n",
    "all_fileids = mt.get_all_files(train_test)\n",
    "\n",
    "###récupération des features précédement extraites\n",
    "features_by_file = mt.lire_json(\"features_by_file.json\")\n",
    "\n",
    "### récupération de la liste des noms des features extraites\n",
    "id_hasard = all_fileids[0]\n",
    "feature_names = features_by_file[id_hasard].keys()\n",
    "\n",
    "###Définition des combinaisons de features\n",
    "\n",
    "feature_sets = {\"all_features\" : feature_names,\n",
    "              \"All_features\"   : [\"L_moyenne_mots\", \"L_min_mots\", \"L_max_mots\", \"nb_affirm\", \"nb_interro\", \"nb_excla\", \"Moyenne_mot_phrase\"]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Création des ARFF\n",
    "try:\n",
    "  os.makedirs(\"arff_files\")#création du dossier pour les ranger\n",
    "  print(\"Dossier 'arff_files' créé\")\n",
    "except:\n",
    "  print(\"Dossier 'arff_files' déjà créé\")\n",
    "  pass\n",
    "\n",
    "for feature_set_name, feature_list in feature_sets.items():\n",
    "  print(\"\\nFeature set : %s\"%feature_set_name)\n",
    "  for dataset, classes in train_test.items():\n",
    "    print(\"  Processing %s set\"%dataset)\n",
    "    filename = \"arff_files/%s__%s.arff\"%(feature_set_name, dataset)\n",
    "    lignes_arff = get_lignes_arff(feature_list, classes, features_by_file)\n",
    "    mt.ecrire(\"\\n\".join(lignes_arff), filename)\n",
    "    print(\"    ->sortie = %s\"%filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
